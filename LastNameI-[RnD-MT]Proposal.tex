%\documentclass[rnd]{mas_proposal}
\documentclass[thesis]{mas_proposal}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\title{Project Proposal Title}
\author{Abhishek Padalkar}
\supervisors{First Supervisor\\Alex Mitrevsky \\ Third Supervisor}
\date{Month 20XX}

% \thirdpartylogo{path/to/your/image}

\begin{document}

\maketitle

\pagestyle{plain}

\chapter{Draft}

\section{Ideas}
\begin{itemize}
	\item Use model, demonstration and reinforcement in one approach.
	\begin{itemize}
		\item Model the constraints on the task
		\item learn DMP and modify it for the constraints
		\item optimize and tune it using reinforcement
	\end{itemize}
	\item Generalize for door opening, sweeping floor, cleaning blackboard and manipulating objects by dragging.
\end{itemize}

\section{Use Cases}
\begin{itemize}
	\item Opening door
	\item Sweeping floor
	\item Cleaning whiteboard
	\item Cleaning window or any flat surface
	\item Object manipulation by dragging
\end{itemize}

\chapter{Introduction}
Most of the real world robotic manipulation task present the need for compliant manipulation, where robot needs to respond to the contact forces while executing the task. Classical planning and control algorithm fail to perform satisfactorily due to the lack of precise model of contact forces and high computational complexity. Various approaches have been proposed to learn compliant manipulation skills with help of Reinforcement Learning (RL). RL, in theory, is very promising when it comes to learn intelligent behaviors in complex environment with high dimensions. But it requires high number of interactions with environment. In model free reinforcement learning, a agent learns the skills by exploring the environment and adopting the parameters which governs the trajectory of the agent in the environment. Learning all the parameters of the policy can be computationally very expensive and might require large number of interactions with the environment. Application of RL to robotics is limited by above reason because large number trials causes wear and tear in mechanical parts and even damage to robot and the environment apart from being time consuming. Model based RL provide the solution to the problem of required high number of interactions with the environment by modeling some part of environment and motions policy. 

On the other hand, task specification approaches like iTasc rely on instantaneous task specification in task frame where constraint on motion are specified in each direction. Such approaches are more practical because of less number of robot interactions and deterministic nature. But some parameters in this case need to be tuned manually which is tedious task and require number of human interventions. 
Kalakrishnan et. al. in \cite{kalakrishnan2011learning} propose a control algorithm by joint RL with intelligent control which learns force policy to open door on top of the specified motion for unlatching and opening opening operation.
We propose to extend above mentioned work by using task specification like iTask and using RL for searching parameters good enough for executing the task. Our approach combines the features from model based manipulation solutions and RL. We propose to bring together the feature of deterministic solution from model based control solutions and self learning capability from RL in one solution.  
\begin{itemize}
	\item ItaSc and limitations
	\item RL and limitations
	\item model based RL
	\item propose the solution
	\item identify switching between primitive policies  
\end{itemize}
\begin{itemize}
	\item In model free reinforcement learning, an agent learns the skills by exploring the environment and adopting the parameters which governs the trajectory of the agent in the environment.
	\item Learning all the parameters of the policy can be computationally very expensive and might require large number of interactions with the environment.
	\item Application of RL to robotics is limited by above reason because large number trials causes wear and tear in mechanical parts and even damage to robot and the environment apart from being time consuming. 
	\item If we can model the environment and constraints on the motion of the robot, we can drastically reduce the number of parameters to be learn to achieve the task.
	\item Use of reinforcement learning to learn these reduced number of parameters can result in near optimal policy for achieving the task.
	\item 
\end{itemize}
\begin{itemize}
    \item Manipulation tasks imply complex contact interactions with an unstructured environment and require a controller that is able to handle force interactions in a meaningful way \cite{kalakrishnan2011learning}.
    \item planning algorithms would require precise dynamics models of the resulting contact interactions. These models are usually unavailable, or so im- precise that the generated plans are unusable\cite{kalakrishnan2011learning}.
    \item A lot of times humans manipulate objects by dragging them on the surface from one place to another. This saves energy because we don't have to lift and place the object. Robots can do the same. 
    \begin{itemize}
    	\item Object manipulation with less energy.
    	\item Pushing objects away to reach for another object in cluttered space.
    \end{itemize}
\end{itemize}

\section{Problem Statement}
\begin{itemize}
    \item We propose to create framework based on task specification and model based reinforcement learning to solve the problem of compliant manipulation for given task in deterministic manner with fewer number of interactions with environment.
    \item We will evaluate our approach based by learning a demo task of opening door. We will evaluate it on the basis of number of interactions needed with environment, deterministic .
\end{itemize}


\chapter{Related Work}
\begin{itemize}
	\item 
    \item What have other people done?
    \item Why is it not sufficient?
\end{itemize}

\section{Section 1}
\section{Section 2}

\chapter{Proposed Solution}

We plan to solve a compliant manipulation task by providing model of the environment and then tuning the task parameters with the help of re-reinforcement learning.

Examples of task modeling: 

\begin{lstlisting}

open_door
{
	f_x = f($\Theta$(environment, robot, task))
	v_y = $v$
	max_acc = $a_{max}$
	max_dec = $a_{dec}$
}until(f(...))

\end{lstlisting}


\begin{lstlisting}

assist_human_to_carry
{
	f_x = f($\Theta$(environment, robot, task))
	v_x = g1(ft_sensor_measurement)
	v_y = g2(ft_sensor_measurement)
	v_z = g3(ft_sensor_measurement)
	damping_x = C1
	damping_y = C1
	damping_z = C1
}until(f2(...))

\end{lstlisting}

Above task specification has the parameters which needs to be tuned either manually or automatically. These parameters will be tuned with model based RL.

\chapter{Project Plan}

\section{Work Packages}
The bare minimum will include the following packages:
\begin{enumerate}
    \item[WP1] Literature Search
    \item[WP2] Experiments
    \item[WP3] Project Report
\end{enumerate}
Keep in mind that depending on your project, you will probably need to add work packages that are more suited to your projects.

\section{Milestones}
\begin{enumerate}
    \item[M1] Literature search
    \item[M2] Experimental setup
    \item[M3] Experimental Analysis
    \item[M4] Report submission
\end{enumerate}

\section{Project Schedule}
Include a gantt chart here. It doesn't have to be detailed, but it should include the milestones you mentioned above.
Make sure to include the writing of your report throughout the whole project, not just at the end.

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{rnd_deliverable_timeline}
    \caption{}
    \label{}
\end{figure}

\section{Deliverables}
\subsection{Minimum Viable}

\begin{itemize}
    \item Survey
    \item Analysis of state of the art
    \item Simple simulated use case
    \item Demo on youBot or Jenny
\end{itemize}

\subsection{Expected}
\begin{itemize}
    \item Comparation of approaches in the robot
\end{itemize}

\subsection{Desired}
\begin{itemize}
    \item Integration to scenario
\end{itemize}


\nocite{*}

\bibliographystyle{plainnat} % Use the plainnat bibliography style
\bibliography{bibliography.bib} % Use the bibliography.bib file as the source of references




\end{document}
