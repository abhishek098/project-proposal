%\documentclass[rnd]{mas_proposal}
\documentclass[thesis]{mas_proposal}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{%
	\parbox{\textwidth}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}\vskip-4pt}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\lstset{frame=lrb,xleftmargin=\fboxsep,xrightmargin=-\fboxsep}

\lstset{
	% numbers=left,
	breaklines=true,
	% backgroundcolor=\color{light-gray},
	tabsize=4,
	% basicstyle=\ttfamily,
	literate={\ \ }{{\ }}1	
}

\title{Compliant Manipulation with Task Specification and Reinforcement Learning}
\author{Abhishek Padalkar}
\supervisors{Paul Pl\"{o}ger\\Sven Schneider \\ Third Supervisor}
\date{\today}

% \thirdpartylogo{path/to/your/image}

\begin{document}

\maketitle

\pagestyle{plain}

\chapter{Misc}
\begin{itemize}
	\item Search predictive power of RL
	\item application of predictive power RL 
\end{itemize}

\chapter{Introduction}
Most of the real world robotic manipulation task present the need for compliant manipulation, where robot needs to respond to the contact forces while executing the task. Classical planning and control algorithm fail to perform satisfactorily due to the lack of precise model of contact forces and high computational complexity. Various approaches have been proposed to learn compliant manipulation skills with help of Reinforcement Learning (RL). RL, in theory, is very promising when it comes to learn intelligent behaviors in complex environment with high dimensions. But it requires high number of interactions with environment. In model free reinforcement learning, a agent learns the skills by exploring the environment and adopting the parameters which governs the trajectory of the agent in the environment. Learning all the parameters of the policy can be computationally very expensive and might require large number of interactions with the environment. Application of RL to robotics is limited by above reason because large number trials causes wear and tear in mechanical parts and even damage to robot and the environment apart from being time consuming. Model based RL provide the solution to the problem of required high number of interactions with the environment by modeling some part of environment and motions policy. 

On the other hand, task specification approaches like iTasc rely on instantaneous task specification in task frame where constraint on motion are specified in each direction. Such approaches are more practical because of less number of robot interactions and deterministic nature. But some parameters in this case need to be tuned manually which is tedious task and require number of human interventions. 
Kalakrishnan et. al. in \cite{kalakrishnan2011learning} propose a control algorithm by joint RL with intelligent control which learns force policy to open door on top of the specified motion for unlatching and opening opening operation.
We propose to extend above mentioned work by using task specification like iTask and using RL for searching parameters good enough for executing the task. Our approach combines the features from model based manipulation solutions and RL. We propose to bring together the feature of deterministic solution from model based control solutions and self learning capability from RL in one solution.  


\section{Problem Statement}
\begin{itemize}
    \item We propose to create a framework based on task specification and model based reinforcement learning to solve the problem of compliant manipulation for given task in deterministic manner with fewer number of interactions with environment.
    \item We will evaluate our approach based by learning a demo task of opening door. We will evaluate it on the basis of number of interactions needed with environment, deterministic .
\end{itemize}


\chapter{Related Work}
\section{Task specification}
In industrial settings, task are usually specified in the form of sequence of primitives \cite{leidner2017cognitive}. These primitives mostly contain point-to-point motion, motion to some default configuration and basic curves. For example, following script taken from FRI interface examples shows the example motion primitives as shown in listing \ref{KRL-sample}. Though this kind of task specification works in controlled industrial environment, it leaves very little scope for external sensory feedback and the task needs to specified in terms of positional goals.

\begin{lstlisting}[label=KRL-sample,caption=KRL code]
INI
PTP HOME ; go to HOME joint configuration
LIN P1   ; linear motion to point P1
LIN P2	 ; linear motion to point P2
\end{lstlisting}

In \cite{leidner2017cognitive}, Leidner presented a representation in the form of action templates describing robot action using symbolic representations and geometric process model as shown in listing \ref{a-template}.  Symbolic representation of the task in PDDL allows classical planners to consider the action in the high level abstract task plan and geometric representation of the task specifies the sequence of low level movement sequences needed to complete the action. Here, geometric representation only specifies discrete motion primitives and not the parametric representation of the motion primitives. Actual parameterization and control is left to low level control module.   

\begin{lstlisting}[label=a-template,caption=Action Template: \_object.pick]
I Symbolic Representation 

'''
(:action _object.pick: 
:parameters (?o - _object ?m - _manipulator ?s - _surface) 
:precondition (and(free ?m) (on ?o ?s)) 
:effect (and(bound ?o ?m) (not(free ?m)) (not(on ?o ?s)))
) 
''' 

II Geometric Representation

def pick(self, manipulator, surface):
''' approach, grasp, and lift an object from a surface '''

graspset = odb.get_property(self.type, 'graspset', manipulator) 
for grasp_candidate in graspset: 
if grasp_candidate in self.history: 
continue
self.history.append(candidate) 
grasp = grasp_candidate 
break

if grasp is None: 
raise RuntimeError('no more alternatives')
else: 
self.grasp = grasp
operations = [
('move_hand', manipulator, g.approach_grasp), 
('plan_to_frame', manipulator, g.approach_frame, self.frame), ('plan_to_frame', manipulator, g.grasp_frame, self.frame), 
('bind', manipulator, self.name), 
('move_hand', manipulator, g.pre_grasp), 
('move_hand', manipulator, g.grasp), 
('move_straight', manipulator, txyz(0, 0, 0.1))
] 
return operations
\end{lstlisting}

In \cite{mason1981compliance}, Mason et. al. presented the idea behind \textit{Task Frame Formalization - TFF} for representing compliant task. Using hybrid control, various control modes are assigned to each axis of the \textit{task frame} or \textit{compliance frame}\cite{nagele2018prototype}. Listing \ref{tff} shows Open Door task taken from \cite{bruyninckx1996specification}. This framework doesn't consider the specification of task or motion quality related parameters like velocity damping or instantaneous sensory inputs. It also specify action to executed \textit{compliantly} but does not specify the how? 

\begin{lstlisting}[label=tff,caption=Task Specification using TFF: Open Door]
move compliantly {
	with task frame directions
	xt: force 0 N
	yt: force 0 N
	zt: velocity v mm/sec
	axt: force 0 Nmm
	ayt: force 0 Nmm
	azt: force 0 Nmm
} until distance > d mm 
\end{lstlisting}


iTaSC developed in \cite{DeSchutter-ijrr2007, DecreBruyninckxDeSchutter2013, decre09}, synthesizes control inputs based on provided task space constraints. It formulates a optimization problem considering provided constraints in the environment. In case of conflicting constraints, constraints are weighted in the optimization problem.  

\chapter{Reinforcement Learning}

Ideally, using reinforcement learning, an agent can find an optimal or near optimal behavior by interacting with the environment. Instead of providing a detailed hand solution, a value function provides the feedback on the task which enables agent to learn a optimal solution.  

\section{Model Free Reinforcement Learning}

\section{Model Based Reinforcement Learning}

In model free reinforcement learning, an agent learns the value function for each action taken, which governs the trajectory of the agent in the environment. Learning all the parameters of the policy can be computationally very expensive and might require large number of interactions with the environment. Application of RL to robotics is limited by above reason because large number trials causes wear and tear in mechanical parts and even damage to robot and the environment apart from being time consuming If we can model the environment and constraints on the motion of the robot, we can drastically reduce the number of parameters to be learn to achieve the task. Use of reinforcement learning to learn these reduced number of parameters can result in near optimal policy for achieving the task. Manipulation tasks imply complex contact interactions with an unstructured environment and require a controller that is able to handle force interactions in a meaningful way \cite{kalakrishnan2011learning}. planning algorithms would require precise dynamics models of the resulting contact interactions. These models are usually unavailable, or so im- precise that the generated plans are unusable\cite{kalakrishnan2011learning}. 

\section{Door Opening}

Opening door is one the most important task for service and rescue robots. Number of studies have been conducted to investigate this challenging problem using different techniques. Number of approaches rely on detail geometric modeling of the door and analytical trajectory generation. Nagatani et.al. \cite{nagatani1995experiment} presented an early approach for opening door by modeling the geometry of the door and then synthesizing trajectory to open door by considering geometrical constraints. To cope with the errors in position control of the early days manipulator used in the experiment, they used a force torque sensor module to achieve compliance with the environment. But this approach highly relies on correct geometric model and accurate execution of designed trajectories.

Approaches presented in \cite{levihn2014using,karayiannidis2012adaptive,niemeyer1997simple} use adaptive control algorithms which synthesize the motion by identifying the environmental constraints. In \cite{karayiannidis2012adaptive}, authors proposed to find the radial direction based on force and torque readings. Door is opened by applying velocity in radial direction. This method is prone to failure in case of uncertain grasp position. Also it considers only the case where door is already unlatched. 

Some of the methods for door opening motor primitives like Dynamic motion primitives for generating trajectories for opening door. Disadvantage of these approaches is that we need to know the goal pose to generate trajectory hence it confines the condition of completing the task to achieving a goal pose. 


\chapter{Proposed Solution}

We plan to solve a compliant manipulation task by providing a task specification and using Reinforcement Learning algorithm for tuning the task specification parameters.

\section{Composition}

\begin{figure}[!h]
	\center{\includegraphics[width=\textwidth]
		{images/composition.png}}
	\caption{\label{fig:composition} Composition}
\end{figure}
Above figure shows the composition of the framework.

\section{Task Specification}
Examples of task specification: 
\begin{lstlisting}[label=open_door_ts,caption=Task specification for opening door]
open_door
{
	with task frame directions
	xt: force 0 N
	yt: force 0 N
	zt: force f($\Theta$(environment, robot, task)) N
	axt: force 0 Nmm
	ayt: force 0 Nmm
	azt: force 0 Nmm
	max_acc = $a_{max}$ m/ss
	max_dec = $a_{dec}$ m/ss
}until(g(...))

\end{lstlisting}


\begin{lstlisting}[label=assist_ts,caption=Task specification for cutting vegetables]

cut_vegetables
{
	with task frame directions
	xt: velocity v(t) m/s
	yt: force 0 N
	zt: force f($\Theta$(environment, robot, task)) N
	axt: force 0 Nmm
	ayt: force 0 Nmm
	azt: force 0 Nmm
	max_acc = $a_{max}$ m/ss
	max_dec = $a_{dec}$ m/ss
}until(g(...))

\end{lstlisting}

Above task specification has the parameters which needs to be tuned either manually or automatically. These parameters will be tuned with model based RL.

\chapter{Project Plan}

\section{Work Packages}
The bare minimum will include the following packages:
\begin{enumerate}
    \item[WP1] Literature search
    \item[WP2] Design of methodology and implementation
    \item[WP3] Experiments
    \item[WP4] Project report
\end{enumerate}

\section{Milestones}
\begin{enumerate}
    \item[M1] Literature search
    \item[m2] Design of methodology and implementation
    \item[M2] Experimental setup
    \item[M3] Experimental analysis
    \item[M4] Report submission
\end{enumerate}

\section{Project Schedule}

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{rnd_deliverable_timeline}
    \caption{}
    \label{}
\end{figure}

\section{Deliverables}
\subsection{Minimum Viable}

\begin{itemize}
    \item Survey of state of the art
    \item Design and implementation of proposed solution
    \item Demo of door opening task
\end{itemize}

\subsection{Expected}
\begin{itemize}
    \item Demo of door opening task with RL
\end{itemize}

\subsection{Desired}
\begin{itemize}
    \item Demo of vegetable cutting task with RL
\end{itemize}


\nocite{*}

\bibliographystyle{plainnat} % Use the plainnat bibliography style
\bibliography{bibliography.bib} % Use the bibliography.bib file as the source of references




\end{document}
